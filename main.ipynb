{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Boilerplate Project\n",
    "\n",
    "This notebook is designed to provide a structured and modular framework for building, evaluating, and testing machine learning models. It simplify the typical machine learning pipeline, including data preprocessing, cross-validation, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Dataset</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define File Path and Parameters\n",
    "\n",
    "Before starting, configure the following parameters to match your dataset and requirements:\n",
    "\n",
    "- **FILE_PATH**: Path to the CSV file containing your dataset.\n",
    "- **TARGET_COL**: Name of the target column in your dataset (i.e., the variable you want to predict).\n",
    "- **RANDOM_SEED**: Random seed to ensure reproducibility across different runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"data/\"  # replace with the path to your CSV file\n",
    "TARGET_COL = \"target\"  # replace with the name of your target column\n",
    "RANDOM_SEED = 42  # random seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset and Extract Features\n",
    "\n",
    "This section is responsible for loading the dataset and preparing the features (`X`) and target (`y`) variables. By the end of this step, the dataset is ready for further preprocessing and modeling. If your data does not contains a ground truth column, consider to build it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data = pd.read_csv(FILE_PATH)\n",
    "    print(f\"Data successfully loaded from {FILE_PATH}.\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"File not found: {FILE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    X = data.drop(columns=[TARGET_COL])\n",
    "    y = data[TARGET_COL]\n",
    "    print(f\"Features (X) shape: {X.shape}\")\n",
    "    print(f\"Target (y) shape: {y.shape}\")\n",
    "except KeyError:\n",
    "    raise KeyError(f\"Target column '{TARGET_COL}' not found in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots\n",
    "\n",
    "Distribution and Heatmap plots for the input data and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x=y, palette='viridis')\n",
    "\n",
    "# Add value counts on top of the bars\n",
    "for i, v in enumerate(pd.Series(y).value_counts()):\n",
    "    plt.text(i, v, str(v), ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "plt.title('Distribution of Classes', fontsize=16)\n",
    "plt.xlabel('Class', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.xticks(rotation=45, fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = X.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, \n",
    "            annot=True, \n",
    "            cmap='coolwarm', \n",
    "            center=0, \n",
    "            fmt='.2f', \n",
    "            square=True, \n",
    "            linewidths=0.5)\n",
    "\n",
    "plt.title('Correlation Heatmap of Features', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Data Pre-processing</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding for Categorical Features\n",
    "\n",
    "In this section, one-hot encoding is applied directly to the categorical features in the dataset. This technique transforms categorical variables into a binary matrix, where each unique category is represented by a binary column.\n",
    "\n",
    "### Steps:\n",
    "1. **Select Categorical Columns**: Identify the columns in the dataset that contain categorical data.\n",
    "2. **One-Hot Encoding**: Use the `OneHotEncoder` from `sklearn` to transform the categorical columns into binary features.\n",
    "3. **Merge Encoded Data**: Replace the original categorical columns with their encoded counterparts in the dataset.\n",
    "\n",
    "This ensures that the categorical features are properly represented for machine learning algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = X.select_dtypes(include=['object', 'category']).columns\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "X_encoded = encoder.fit_transform(X[categorical_columns])\n",
    "encoded_columns = encoder.get_feature_names_out(categorical_columns)\n",
    "X_encoded_df = pd.DataFrame(X_encoded, columns=encoded_columns, index=X.index)\n",
    "X = pd.concat([X.drop(columns=categorical_columns), X_encoded_df], axis=1)\n",
    "\n",
    "if y.dtype in ['object', 'category']:\n",
    "    if len(y.unique()) > 2:\n",
    "        encoder_y = OneHotEncoder(sparse_output=False)\n",
    "        y_encoded = encoder_y.fit_transform(y.values.reshape(-1, 1))\n",
    "        encoded_columns_y = encoder_y.get_feature_names_out([TARGET_COL])\n",
    "        y = pd.DataFrame(y_encoded, columns=encoded_columns_y, index=y.index)\n",
    "    else:\n",
    "        encoder_y = LabelEncoder()\n",
    "        y = pd.Series(encoder_y.fit_transform(y), index=y.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Missing Values\n",
    "\n",
    "This section implements a comprehensive strategy for dealing with missing values in the dataset. The approach follows best practices in data preprocessing and includes multiple handling methods based on the nature and extent of missingness.\n",
    "\n",
    "### Missing Value Analysis\n",
    "First, we create a summary of missing values across all columns, including:\n",
    "- Total count of missing values per column\n",
    "- Percentage of missing values per column\n",
    "\n",
    "### Handling Strategy\n",
    "The code implements different strategies based on two key factors:\n",
    "1. **Percentage of Missing Values**:\n",
    "  - 0%: No action needed\n",
    "  - \\>70%: Drop the column entirely\n",
    "  - 5-70%: Create missing indicator + imputation  \n",
    "  - <5%: Simple imputation\n",
    "\n",
    "2. **Data Type**:\n",
    "  - **Numerical Columns**:\n",
    "    - Imputation using median values\n",
    "    - Missing indicators for columns with >5% missing\n",
    "  \n",
    "  - **Categorical Columns**:\n",
    "    - Imputation using mode (most frequent value) for low missingness\n",
    "    - Imputation with 'Missing' category for higher missingness\n",
    "    - Missing indicators for columns with >5% missing\n",
    "\n",
    "### Quality Control\n",
    "The section includes verification steps:\n",
    "- Prints the dataset shape before and after handling missing values\n",
    "- Updates the feature matrix (X) and target vector (y)\n",
    "- Includes an assertion to verify that no missing values remain in the dataset\n",
    "\n",
    "This approach ensures robust handling of missing data while preserving important information about missingness patterns through indicator variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_stats = pd.DataFrame({\n",
    "    'Total Missing': X.isnull().sum(),\n",
    "    'Percent Missing': (X.isnull().sum() / len(X) * 100).round(2)\n",
    "})\n",
    "missing_stats[missing_stats['Total Missing'] > 0].sort_values('Percent Missing', ascending=False)\n",
    "\n",
    "if len(missing_stats) > 0:\n",
    "    print(\"Missing Value Statistics:\")\n",
    "    print(missing_stats)\n",
    "    print(\"\\nShape before handling missing values:\", X.shape)\n",
    "    \n",
    "    for column in X.columns:\n",
    "        missing_pct = (X[column].isnull().sum() / len(X)) * 100\n",
    "        \n",
    "        if missing_pct == 0:\n",
    "            continue\n",
    "        \n",
    "        elif missing_pct > 70:\n",
    "            X = X.drop(columns=[column])\n",
    "            print(f\"\\nDropped column '{column}' with {missing_pct:.1f}% missing values\")\n",
    "        \n",
    "        elif pd.api.types.is_numeric_dtype(X[column]):\n",
    "            if missing_pct < 5:\n",
    "                X[column] = X[column].fillna(X[column].median())\n",
    "                print(f\"\\nImputed '{column}' with median\")\n",
    "            else:\n",
    "                X[f'{column}_missing'] = X[column].isnull().astype(int)\n",
    "                X[column] = X[column].fillna(X[column].median())\n",
    "                print(f\"\\nCreated missing indicator and imputed '{column}' with median\")\n",
    "        \n",
    "        else:\n",
    "            if missing_pct < 5:\n",
    "                X[column] = X[column].fillna(X[column].mode()[0])\n",
    "                print(f\"\\nImputed '{column}' with mode\")\n",
    "            else:\n",
    "                X[f'{column}_missing'] = X[column].isnull().astype(int)\n",
    "                X[column] = X[column].fillna('Missing')\n",
    "                print(f\"\\nCreated missing indicator and imputed '{column}' with 'Missing' category\")\n",
    "    \n",
    "    print(\"\\nShape after handling missing values:\", X.shape)\n",
    "    \n",
    "else:\n",
    "    print(\"No missing values found in the dataset.\")\n",
    "\n",
    "assert X.isnull().sum().sum() == 0, \"Missing values still present in the dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection: Classification and Regression\n",
    "\n",
    "This section defines the models that will be used in the pipeline. Different models are provided to support both classification and regression tasks, depending on the type of problem you are solving.\n",
    "\n",
    "### Classification Models\n",
    "- **Random Forest Classifier**\n",
    "- **Support Vector Classifier (SVC)**\n",
    "- **XGBoost Classifier**\n",
    "- **MLP Classifier**\n",
    "\n",
    "### Regression Models\n",
    "- **Linear Regression**\n",
    "- **Ridge Regression**\n",
    "- **Random Forest Regressor**\n",
    "- **Support Vector Regressor (SVR)**\n",
    "- **Gradient Boosting Regressor**\n",
    "\n",
    "### Hyperparameter Optimization\n",
    "For each model, a predefined grid of hyperparameters is provided to facilitate tuning using cross-validation. Adjust the hyperparameters as needed for your specific use case, depending on the dataset and computational resources.\n",
    "\n",
    "By selecting and fine-tuning these models, you can evaluate their performance and determine the best approach for your problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(random_state=RANDOM_SEED)\n",
    "\n",
    "grid_params = {\n",
    "    'model__n_estimators': [50, 100, 200],\n",
    "    'model__max_depth': [5, 10, 20],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(random_state=RANDOM_SEED)\n",
    "\n",
    "grid_params = {\n",
    "    'model__C': [0.1, 1, 10],\n",
    "    'model__kernel': ['linear', 'rbf', 'poly'],\n",
    "    'model__gamma': ['scale', 'auto'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(random_state=RANDOM_SEED)\n",
    "\n",
    "grid_params = {\n",
    "    'model__n_estimators': [50, 100, 200],\n",
    "    'model__learning_rate': [0.01, 0.1, 0.3],\n",
    "    'model__max_depth': [3, 5, 7],\n",
    "    'model__subsample': [0.8, 1.0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier(random_state=RANDOM_SEED)\n",
    "\n",
    "grid_params = {\n",
    "    'model__hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "    'model__activation': ['relu', 'tanh'],\n",
    "    'model__solver': ['adam', 'sgd'],\n",
    "    'model__alpha': [0.0001, 0.001, 0.01],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "\n",
    "grid_params = {\n",
    "    'model__fit_intercept': [True, False],\n",
    "    'model__normalize': [True, False],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lasso(random_state=RANDOM_SEED)\n",
    "\n",
    "grid_params = {\n",
    "    'model__alpha': [0.1, 1, 10],\n",
    "    'model__max_iter': [1000, 5000],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(random_state=RANDOM_SEED)\n",
    "\n",
    "grid_params = {\n",
    "    'model__n_estimators': [50, 100, 200],\n",
    "    'model__max_depth': [5, 10, 20],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVR()\n",
    "\n",
    "grid_params = {\n",
    "    'model__C': [0.1, 1, 10],\n",
    "    'model__kernel': ['linear', 'rbf', 'poly'],\n",
    "    'model__gamma': ['scale', 'auto'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor(random_state=RANDOM_SEED)\n",
    "\n",
    "grid_params = {\n",
    "    'model__n_estimators': [50, 100, 200],\n",
    "    'model__learning_rate': [0.01, 0.1, 0.3],\n",
    "    'model__max_depth': [3, 5, 7],\n",
    "    'model__subsample': [0.8, 1.0],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "This section defines two proposed preprocessing pipelines for preparing data before feeding it into the model. Both pipelines aim to normalize the data and perform dimensionality reduction or feature selection, depending on the requirements of the task.\n",
    "\n",
    "### Notes\n",
    "These pipelines are modular, allowing you to replace the classifier (`model`) with any supported model. Choose the pipeline based on the nature of your dataset and the specific goals of your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 1: Feature Selection with `SelectKBest`\n",
    "1. **Normalization**: Uses `StandardScaler` to normalize the data.\n",
    "2. **Feature Selection**: Selects the top `NUM_SELECTED_FEATURES` (default: 10) using ANOVA F-statistic (`SelectKBest` with `f_classif`).\n",
    "3. **Classification**: The selected features are passed to the chosen model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SELECTED_FEATURES = 2\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),                                            \n",
    "    ('feature_selection', SelectKBest(f_classif,\n",
    "                                      k=NUM_SELECTED_FEATURES)),  \n",
    "    ('model', model)                                                    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 2: Dimensionality Reduction with `PCA`\n",
    "1. **Normalization**: Uses `StandardScaler` to normalize the data.\n",
    "2. **Principal Component Analysis (PCA)**: Reduces the dimensionality of the data, retaining `VARIANCE` (default: 95%) of the original variance.\n",
    "3. **Classification**: The transformed features are passed to the chosen model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIANCE = 0.95\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),         \n",
    "    ('pca', PCA(n_components=VARIANCE)),  \n",
    "    ('model', model)                 \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Train-Test Splits\n",
    "\n",
    "Define the proportions and parameters for splitting the dataset into training and testing sets:\n",
    "\n",
    "- **TRAIN_SIZE**: Proportion of the dataset to be used for training in each split (e.g., 0.8 indicates 80% for training and 20% for testing).\n",
    "- **NUM_SPLITS**: Number of splits to perform during cross-validation.\n",
    "\n",
    "These parameters allow you to control the size and frequency of the train-test splits, ensuring robust evaluation of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 0.8  \n",
    "NUM_FOLD = 5    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Classification</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Hyperparameter Tuning\n",
    "\n",
    "This section involves splitting the dataset, performing hyperparameter tuning using cross-validation, and evaluating the best model on the test set.\n",
    "\n",
    "### Train-Test Split\n",
    "- The dataset is split into training and testing sets using `train_test_split` with a stratified split to maintain the class distribution.\n",
    "- The `TRAIN_SIZE` parameter defines the proportion of data used for training.\n",
    "\n",
    "### Hyperparameter Tuning with Grid Search\n",
    "- A `GridSearchCV` object is used to tune the hyperparameters of the pipeline.\n",
    "- Cross-validation is performed with `NUM_FOLD` splits to find the best combination of hyperparameters.\n",
    "- The results of the cross-validation, including mean and standard deviation of accuracy scores for each parameter set, are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    train_size=TRAIN_SIZE, \n",
    "                                                    stratify=y, \n",
    "                                                    random_state=RANDOM_SEED)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=pipeline,\n",
    "                           param_grid=grid_params,\n",
    "                           scoring='accuracy',\n",
    "                           cv=NUM_FOLD,\n",
    "                           n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "mean_cv_scores = cv_results['mean_test_score']\n",
    "std_cv_scores = cv_results['std_test_score']\n",
    "\n",
    "print(\"Cross-Validation Results:\")\n",
    "for i, (mean, std) in enumerate(zip(mean_cv_scores, std_cv_scores)):\n",
    "    print(f\"Parameter set {i+1}: Mean Accuracy = {mean:.4f}, Std = {std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation on Test Data\n",
    "- The best pipeline from the grid search is used to predict the test set.\n",
    "- Evaluation metrics include:\n",
    "  - **Accuracy**: Overall performance of the model.\n",
    "  - **Classification Report**: Precision, recall, F1-score, and support for each class.\n",
    "  - **Confusion Matrix**: Breakdown of true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "By following this process, you can select the best model configuration and validate its performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_pipeline.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=[f\"Class {i}\" for i in range(len(conf_matrix))],\n",
    "            yticklabels=[f\"Class {i}\" for i in range(len(conf_matrix))])\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Regression</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Hyperparameter Tuning\n",
    "\n",
    "This section focuses on splitting the dataset, performing hyperparameter tuning using cross-validation, and evaluating the best model on the test set for regression tasks.\n",
    "\n",
    "### Train-Test Split\n",
    "- The dataset is split into training and testing sets using `train_test_split`.\n",
    "- The `TRAIN_SIZE` parameter specifies the proportion of data to use for training.\n",
    "\n",
    "### Hyperparameter Tuning with Grid Search\n",
    "- A `GridSearchCV` object is used to perform hyperparameter tuning for the pipeline.\n",
    "- Cross-validation is conducted with `NUM_FOLD` splits to identify the optimal hyperparameter configuration.\n",
    "- The mean and standard deviation of the negative Mean Squared Error (MSE) for each parameter set are reported. Negative MSE is converted to positive for interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    train_size=TRAIN_SIZE, \n",
    "                                                    random_state=RANDOM_SEED)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=pipeline,\n",
    "                           param_grid=grid_params,\n",
    "                           scoring='neg_mean_squared_error',  # Use a regression metric\n",
    "                           cv=NUM_FOLD,\n",
    "                           n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "mean_cv_scores = -cv_results['mean_test_score'] \n",
    "std_cv_scores = cv_results['std_test_score']\n",
    "\n",
    "print(\"Cross-Validation Results:\")\n",
    "for i, (mean, std) in enumerate(zip(mean_cv_scores, std_cv_scores)):\n",
    "    print(f\"Parameter set {i+1}: Mean MSE = {mean:.4f}, Std = {std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation on Test Data\n",
    "\n",
    "- The best pipeline from the grid search is used to predict the test set.\n",
    "- Evaluation metrics include:\n",
    "  - **Mean Squared Error (MSE)**: Measures the average squared difference between predicted and actual values. Lower values indicate better model performance.\n",
    "  - **Mean Absolute Error (MAE)**: Measures the average absolute difference between predicted and actual values, providing an interpretable error metric in the same unit as the target variable.\n",
    "  - **R² Score**: Represents the proportion of variance in the target variable explained by the model. Values closer to 1 indicate a better fit.\n",
    "\n",
    "This process allows for a thorough assessment of the model’s performance on unseen data, ensuring the selected pipeline provides reliable predictions for regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "y_pred = best_pipeline.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nTest Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Test Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Test R² Score: {r2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
